{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of classification_model_performance.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1xx5Hnnt5QZ"
      },
      "source": [
        "# Classification Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1mGRqfat9Me"
      },
      "source": [
        "!git clone https://github.com/s7s/machine_learning_1.git\n",
        "%cd  machine_learning_1/classification_model_performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMucOMUDt5Qb"
      },
      "source": [
        "import pandas\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ2NIzSQt5Qc"
      },
      "source": [
        "## Loading dataset\n",
        "\n",
        "First, we use pandas to load the dataset from a csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ejz9_M2pt5Qr"
      },
      "source": [
        "data = pandas.read_csv('./preprocessed_titanic_data.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUwjAMGit5Qs"
      },
      "source": [
        "### Features-labels split and train-validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwr-qv1Nt5Qs"
      },
      "source": [
        "features = data.drop([\"Survived\"], axis=1)\n",
        "labels = data[\"Survived\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHa8SswXt5Qs"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTFZuYFEt5Qs"
      },
      "source": [
        "# remark: we fix random_state the end, to make sure we always get the same split\n",
        "features_train, features_validation_test, labels_train, labels_validation_test = train_test_split(\n",
        "    features, labels, test_size=0.4, random_state=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzkz8cBQt5Qt"
      },
      "source": [
        "features_validation, features_test, labels_validation, labels_test = train_test_split(\n",
        "    features_validation_test, labels_validation_test, test_size=0.5, random_state=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6-n0b0t5Qt"
      },
      "source": [
        "print(len(features_train))\n",
        "print(len(features_validation))\n",
        "print(len(features_test))\n",
        "print(len(labels_train))\n",
        "print(len(labels_validation))\n",
        "print(len(labels_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gofLaM4Qt5Qt"
      },
      "source": [
        "### Training model on our dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O0OyklPt5Qt"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(features_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyCPB2Wdt5Qv"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "#### Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuMO4p7Ct5Qv"
      },
      "source": [
        "print(\"Scores of the model\")\n",
        "# use model score to print model accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awkH73B71LXk"
      },
      "source": [
        "#### False Positives, False Negatives and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx6Dl63d1WKX"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# use sklearn confusion_matrix, ConfusionMatrixDisplay to plot the confusion matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F8oCGoyt5Qv"
      },
      "source": [
        "#### Recall, Precision and F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UenvSyPZt5Qw"
      },
      "source": [
        "from sklearn.metrics import f1_score,recall_score,precision_score\n",
        "# use sklearn f1_score,recall_score,precision_score to print the f1_score,recall_score,precision_score\n",
        "# don't use average parameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zLuCLSGCWA2"
      },
      "source": [
        "### ROC and AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usIXbGEN0OoB"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve\n",
        "# use sklearn plot_roc_curve to plot the ROC and AUC\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}